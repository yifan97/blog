---
layout: post
current: post
cover: assets/images/cover-img/c14.jpg
navigation: True
title: MADE-masked autoencoder for density estimation
date: 2019-07-28
comments: true
---


**This article serves as a note after reading this <a href="https://arxiv.org/pdf/1502.03509.pdf" target="_blank"  style="color: #0074D9;">paper</a>.**

------------------

In previous post, I have explained what is autoencoder. So basically, autoencoder is used for generating or reconstructing data by finding latent variables. However, it is not suitable for density estimation. In this paper, the author proposed a autoregressive model to teawk autoencoder to enable density estimation. 

We know that by chain rule, the joint probability of data $$p(\mathbf(x))$$ can be expressed as:

$$ p(\mathbf(x)) = \prod_{k=1}^n p(x_k|x_{1:k-1}) $$

Apparently, autoencoder cannot do this job because it isa fully connected neural net, meaning that each variable $$x_k$$ depends on all other $$k-1$$ variables. This is a violation of above conditional requirements. So a natural questions is：can we teawk autoencoder so that its output units predict the n conditional distributions instead of reconstructing the n input? That being said, the $$k^th$$ output should only depends on previous $$k-1$$ inputs.

For example, $$\hat x_3$$ should **not** depend on $$x_3$$ and $$x_4$$

<img src="assets/images/MADE/MADE-1.jpg" alt="MADE-1" style="width: 50%;">


To accomplish this, the author used masked weights that mask out some connections so that conditional requirements are satisfied.


Here is a detailed implementation of MADE:

- Assume some ordering of the input and simply label them from 1 to n
- Randomly assign each hidden unit a number from 1 to n-1 which indicated the maximum number of connections respectively
- Implement the mask by taking weight matrices and apply appropriate binary masks to them based on the label of each unit

<img src="assets/images/MADE/MADE-2.jpg" alt="MADE-2" style="width: 50%; float: left;">
<img src="assets/images/MADE/MADE-3.jpg" alt="MADE-3" style="width: 50%;">

If you trace from the outputs, you will find out that the conditional requirements are indeed satisfied. 

This model is designed for generation not for abstraction. They way we use this to generate data follows the following steps:

- Sample a value of $$x_1$$, usually from Gaussian
- Feed $$x_1$$ to the network and compute 
$$p(x_2|x_1)$$ 
- Sample a value of $$x_2$$ from the computed 
$$p(x_2|x_1)$$
- Feed $$x_2$$ to the network and repeat until you generate all variables


In the paper, the author proposed two ways to train this mdoel: **Order-agnostic training** and **Connectivity-agnostic training**

**Order-agnostic training**

Intuitively, we assume that the conditionals modelled by MADE is are consistent with natural ordering of the dimensions of $$\mathbf(x)$$. However, we might be interested in modelling conditionals associated with an arbitraty ordering of the input's dimensions. We call this non-natural *ordering order-agnostic training*. It can be achieved by sampling an ordering before each stochastic/minibatch gradient update of the model. 

**Connectivity-agnostic training**

One advantage of order-agnostic training is that it effectively allows us to train as many models as there are orderings, using a common set of parameters. In MADE, in addition to choosing an ordering, we also have to choose each hidden unit’s connectivity constraint ml
(k). Thus, we could imaging training MADE to also be agnostic of the connectivity pattern generated by these constraints. To achieve this, instead of sampling the values of ml (k) for all units and layers once and for all before training, we actually resample them for each training example or minibatch